{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a7356fae",
   "metadata": {},
   "source": [
    "## News Category prediction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a4680666",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing necessary libraries...\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# for splitting the data...\n",
    "from sklearn.model_selection import train_test_split \n",
    "\n",
    "# for modelling...\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier,GradientBoostingClassifier,BaggingClassifier,AdaBoostClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# for evaluate the model...\n",
    "from sklearn.metrics import accuracy_score,confusion_matrix,classification_report\n",
    "\n",
    "# for ignoring warnings...\n",
    "import warnings \n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "16a8b23f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>headline</th>\n",
       "      <th>links</th>\n",
       "      <th>short_description</th>\n",
       "      <th>keywords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>WELLNESS</td>\n",
       "      <td>143 Miles in 35 Days: Lessons Learned</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/running-l...</td>\n",
       "      <td>Resting is part of training. I've confirmed wh...</td>\n",
       "      <td>running-lessons</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>WELLNESS</td>\n",
       "      <td>Talking to Yourself: Crazy or Crazy Helpful?</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/talking-t...</td>\n",
       "      <td>Think of talking to yourself as a tool to coac...</td>\n",
       "      <td>talking-to-yourself-crazy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>WELLNESS</td>\n",
       "      <td>Crenezumab: Trial Will Gauge Whether Alzheimer...</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/crenezuma...</td>\n",
       "      <td>The clock is ticking for the United States to ...</td>\n",
       "      <td>crenezumab-alzheimers-disease-drug</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>WELLNESS</td>\n",
       "      <td>Oh, What a Difference She Made</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/meaningfu...</td>\n",
       "      <td>If you want to be busy, keep trying to be perf...</td>\n",
       "      <td>meaningful-life</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>WELLNESS</td>\n",
       "      <td>Green Superfoods</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/green-sup...</td>\n",
       "      <td>First, the bad news: Soda bread, corned beef a...</td>\n",
       "      <td>green-superfoods</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   category                                           headline  \\\n",
       "0  WELLNESS              143 Miles in 35 Days: Lessons Learned   \n",
       "1  WELLNESS       Talking to Yourself: Crazy or Crazy Helpful?   \n",
       "2  WELLNESS  Crenezumab: Trial Will Gauge Whether Alzheimer...   \n",
       "3  WELLNESS                     Oh, What a Difference She Made   \n",
       "4  WELLNESS                                   Green Superfoods   \n",
       "\n",
       "                                               links  \\\n",
       "0  https://www.huffingtonpost.com/entry/running-l...   \n",
       "1  https://www.huffingtonpost.com/entry/talking-t...   \n",
       "2  https://www.huffingtonpost.com/entry/crenezuma...   \n",
       "3  https://www.huffingtonpost.com/entry/meaningfu...   \n",
       "4  https://www.huffingtonpost.com/entry/green-sup...   \n",
       "\n",
       "                                   short_description  \\\n",
       "0  Resting is part of training. I've confirmed wh...   \n",
       "1  Think of talking to yourself as a tool to coac...   \n",
       "2  The clock is ticking for the United States to ...   \n",
       "3  If you want to be busy, keep trying to be perf...   \n",
       "4  First, the bad news: Soda bread, corned beef a...   \n",
       "\n",
       "                             keywords  \n",
       "0                     running-lessons  \n",
       "1           talking-to-yourself-crazy  \n",
       "2  crenezumab-alzheimers-disease-drug  \n",
       "3                     meaningful-life  \n",
       "4                    green-superfoods  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the data  news categories data...\n",
    "news = pd.read_csv('NewsCategorizer.csv')\n",
    "news.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e00b121c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 5)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b7e92e4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WELLNESS          5000\n",
       "POLITICS          5000\n",
       "ENTERTAINMENT     5000\n",
       "TRAVEL            5000\n",
       "STYLE & BEAUTY    5000\n",
       "PARENTING         5000\n",
       "FOOD & DRINK      5000\n",
       "WORLD NEWS        5000\n",
       "BUSINESS          5000\n",
       "SPORTS            5000\n",
       "Name: category, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news.category.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d9fb53c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "category                0\n",
       "headline                0\n",
       "links                   0\n",
       "short_description       0\n",
       "keywords             2668\n",
       "dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a2a8927a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>headline</th>\n",
       "      <th>links</th>\n",
       "      <th>short_description</th>\n",
       "      <th>keywords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>WELLNESS</td>\n",
       "      <td>Hotels Let Guests Sleep In On Sundays</td>\n",
       "      <td>https://www.huffingtonpost.comhttp://www.usato...</td>\n",
       "      <td>Hotels are encouraging their guests to sleep i...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>WELLNESS</td>\n",
       "      <td>Auriculotherapy: Penelope Cruz Sports Acupunct...</td>\n",
       "      <td>https://www.huffingtonpost.comhttp://abcnews.g...</td>\n",
       "      <td>The swath of tiny studs in Penelope Cruz's ear...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>WELLNESS</td>\n",
       "      <td>Hormone Therapy 'Not Recommended' By Governmen...</td>\n",
       "      <td>https://www.huffingtonpost.comhttp://www.cnn.c...</td>\n",
       "      <td>The U.S. Preventive Services Task Force says m...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>WELLNESS</td>\n",
       "      <td>7 Ways To Fall Asleep Faster</td>\n",
       "      <td>https://www.huffingtonpost.comhttp://www.daily...</td>\n",
       "      <td>Can't get enough z's? Try these tips so you ca...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>WELLNESS</td>\n",
       "      <td>Doctors Say Changes In Wheat Do Not Explain Ri...</td>\n",
       "      <td>https://www.huffingtonpost.comhttp://www.npr.o...</td>\n",
       "      <td>It's true that about 40 years ago wheat breede...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49971</th>\n",
       "      <td>SPORTS</td>\n",
       "      <td>Oakland Athletics Vs. Detroit Tigers: ALDS Gam...</td>\n",
       "      <td>https://www.huffingtonpost.comhttp://bleacherr...</td>\n",
       "      <td>Game 1 of the American League Divisional Serie...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49980</th>\n",
       "      <td>SPORTS</td>\n",
       "      <td>Tiger Woods On Turning 40 And His Private Stru...</td>\n",
       "      <td>https://www.huffingtonpost.comhttp://pubx.co/k...</td>\n",
       "      <td>Tiger Woods was raised to be a champion. Groom...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49982</th>\n",
       "      <td>SPORTS</td>\n",
       "      <td>100 Hottest NFL Cheerleaders | Bleacher Report</td>\n",
       "      <td>https://www.huffingtonpost.comhttp://bleacherr...</td>\n",
       "      <td>Bringing you the 100 Hottest Cheerleaders in t...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49994</th>\n",
       "      <td>SPORTS</td>\n",
       "      <td>Tiger Woods And Rory McIlroy, Honda Classic Li...</td>\n",
       "      <td>https://www.huffingtonpost.comhttp://golfweek....</td>\n",
       "      <td>Tiger Woods is looking to springboard off a 5-...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49998</th>\n",
       "      <td>SPORTS</td>\n",
       "      <td>Why Jake Plummer And Other NFL Players Are Pus...</td>\n",
       "      <td>https://www.huffingtonpost.comhttp://extras.de...</td>\n",
       "      <td>They believe CBD could be an alternative to po...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2668 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       category                                           headline  \\\n",
       "51     WELLNESS              Hotels Let Guests Sleep In On Sundays   \n",
       "57     WELLNESS  Auriculotherapy: Penelope Cruz Sports Acupunct...   \n",
       "59     WELLNESS  Hormone Therapy 'Not Recommended' By Governmen...   \n",
       "67     WELLNESS                       7 Ways To Fall Asleep Faster   \n",
       "101    WELLNESS  Doctors Say Changes In Wheat Do Not Explain Ri...   \n",
       "...         ...                                                ...   \n",
       "49971    SPORTS  Oakland Athletics Vs. Detroit Tigers: ALDS Gam...   \n",
       "49980    SPORTS  Tiger Woods On Turning 40 And His Private Stru...   \n",
       "49982    SPORTS     100 Hottest NFL Cheerleaders | Bleacher Report   \n",
       "49994    SPORTS  Tiger Woods And Rory McIlroy, Honda Classic Li...   \n",
       "49998    SPORTS  Why Jake Plummer And Other NFL Players Are Pus...   \n",
       "\n",
       "                                                   links  \\\n",
       "51     https://www.huffingtonpost.comhttp://www.usato...   \n",
       "57     https://www.huffingtonpost.comhttp://abcnews.g...   \n",
       "59     https://www.huffingtonpost.comhttp://www.cnn.c...   \n",
       "67     https://www.huffingtonpost.comhttp://www.daily...   \n",
       "101    https://www.huffingtonpost.comhttp://www.npr.o...   \n",
       "...                                                  ...   \n",
       "49971  https://www.huffingtonpost.comhttp://bleacherr...   \n",
       "49980  https://www.huffingtonpost.comhttp://pubx.co/k...   \n",
       "49982  https://www.huffingtonpost.comhttp://bleacherr...   \n",
       "49994  https://www.huffingtonpost.comhttp://golfweek....   \n",
       "49998  https://www.huffingtonpost.comhttp://extras.de...   \n",
       "\n",
       "                                       short_description keywords  \n",
       "51     Hotels are encouraging their guests to sleep i...      NaN  \n",
       "57     The swath of tiny studs in Penelope Cruz's ear...      NaN  \n",
       "59     The U.S. Preventive Services Task Force says m...      NaN  \n",
       "67     Can't get enough z's? Try these tips so you ca...      NaN  \n",
       "101    It's true that about 40 years ago wheat breede...      NaN  \n",
       "...                                                  ...      ...  \n",
       "49971  Game 1 of the American League Divisional Serie...      NaN  \n",
       "49980  Tiger Woods was raised to be a champion. Groom...      NaN  \n",
       "49982  Bringing you the 100 Hottest Cheerleaders in t...      NaN  \n",
       "49994  Tiger Woods is looking to springboard off a 5-...      NaN  \n",
       "49998  They believe CBD could be an alternative to po...      NaN  \n",
       "\n",
       "[2668 rows x 5 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news[news['keywords'].isna()==True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2db91c32",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f933ab38",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "01e1c9c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "news['label'] = news['category'].map({'WELLNESS':0, 'POLITICS':1, 'ENTERTAINMENT':2, 'TRAVEL':3,\n",
    "       'STYLE & BEAUTY':4, 'PARENTING':5, 'FOOD & DRINK':6, 'WORLD NEWS':7,\n",
    "       'BUSINESS':8, 'SPORTS':9})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bb6f8124",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>headline</th>\n",
       "      <th>links</th>\n",
       "      <th>short_description</th>\n",
       "      <th>keywords</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>WELLNESS</td>\n",
       "      <td>143 Miles in 35 Days: Lessons Learned</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/running-l...</td>\n",
       "      <td>Resting is part of training. I've confirmed wh...</td>\n",
       "      <td>running-lessons</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>WELLNESS</td>\n",
       "      <td>Talking to Yourself: Crazy or Crazy Helpful?</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/talking-t...</td>\n",
       "      <td>Think of talking to yourself as a tool to coac...</td>\n",
       "      <td>talking-to-yourself-crazy</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>WELLNESS</td>\n",
       "      <td>Crenezumab: Trial Will Gauge Whether Alzheimer...</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/crenezuma...</td>\n",
       "      <td>The clock is ticking for the United States to ...</td>\n",
       "      <td>crenezumab-alzheimers-disease-drug</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>WELLNESS</td>\n",
       "      <td>Oh, What a Difference She Made</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/meaningfu...</td>\n",
       "      <td>If you want to be busy, keep trying to be perf...</td>\n",
       "      <td>meaningful-life</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>WELLNESS</td>\n",
       "      <td>Green Superfoods</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/green-sup...</td>\n",
       "      <td>First, the bad news: Soda bread, corned beef a...</td>\n",
       "      <td>green-superfoods</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   category                                           headline  \\\n",
       "0  WELLNESS              143 Miles in 35 Days: Lessons Learned   \n",
       "1  WELLNESS       Talking to Yourself: Crazy or Crazy Helpful?   \n",
       "2  WELLNESS  Crenezumab: Trial Will Gauge Whether Alzheimer...   \n",
       "3  WELLNESS                     Oh, What a Difference She Made   \n",
       "4  WELLNESS                                   Green Superfoods   \n",
       "\n",
       "                                               links  \\\n",
       "0  https://www.huffingtonpost.com/entry/running-l...   \n",
       "1  https://www.huffingtonpost.com/entry/talking-t...   \n",
       "2  https://www.huffingtonpost.com/entry/crenezuma...   \n",
       "3  https://www.huffingtonpost.com/entry/meaningfu...   \n",
       "4  https://www.huffingtonpost.com/entry/green-sup...   \n",
       "\n",
       "                                   short_description  \\\n",
       "0  Resting is part of training. I've confirmed wh...   \n",
       "1  Think of talking to yourself as a tool to coac...   \n",
       "2  The clock is ticking for the United States to ...   \n",
       "3  If you want to be busy, keep trying to be perf...   \n",
       "4  First, the bad news: Soda bread, corned beef a...   \n",
       "\n",
       "                             keywords  label  \n",
       "0                     running-lessons      0  \n",
       "1           talking-to-yourself-crazy      0  \n",
       "2  crenezumab-alzheimers-disease-drug      0  \n",
       "3                     meaningful-life      0  \n",
       "4                    green-superfoods      0  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2445abbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import regex as re\n",
    "from bs4 import BeautifulSoup\n",
    "import spacy\n",
    "import nltk\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "import re\n",
    "import unicodedata\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "\n",
    "def remove_accented_chars(text):\n",
    "    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "    return text\n",
    "\n",
    "def remove_special_characters(text, remove_digits=False):\n",
    "    pattern = r'[^a-zA-z0-9\\s]' if not remove_digits else r'[^a-zA-z\\s]'\n",
    "    text = re.sub(pattern, '', text)\n",
    "    return text\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    text = nlp(text)\n",
    "    text = ' '.join([word.lemma_ if word.lemma_ != '-PRON-' else word.text for word in text])\n",
    "    return text\n",
    "\n",
    "\n",
    "# Cleaning data\n",
    "def normalize_corpus(corpus, contraction_expansion=True,\n",
    "                     accented_char_removal=True, text_lower_case=True, \n",
    "                     text_lemmatization=True, special_char_removal=True, \n",
    "                      remove_digits=True):\n",
    "    \n",
    "    normalized_corpus = []\n",
    "    # normalize each document in the corpus\n",
    "    for doc in corpus:\n",
    "        # remove accented characters\n",
    "        if accented_char_removal:\n",
    "            doc = remove_accented_chars(doc)\n",
    "        # lowercase the text    \n",
    "        if text_lower_case:\n",
    "            doc = doc.lower()\n",
    "        # remove extra newlines\n",
    "        doc = re.sub(r'[\\r|\\n|\\r\\n]+', ' ',doc)\n",
    "        # lemmatize text\n",
    "        if text_lemmatization:\n",
    "            doc = lemmatize_text(doc)\n",
    "        # remove special characters and\\or digits    \n",
    "        if special_char_removal:\n",
    "            # insert spaces between special characters to isolate them    \n",
    "            special_char_pattern = re.compile(r'([{.(-)!}])')\n",
    "            doc = special_char_pattern.sub(\" \\\\1 \", doc)\n",
    "            doc = remove_special_characters(doc, remove_digits=remove_digits)  \n",
    "        # remove extra whitespace\n",
    "        doc = re.sub(' +', ' ', doc)\n",
    "        \n",
    "            \n",
    "        normalized_corpus.append(doc)\n",
    "        \n",
    "    return normalized_corpus\n",
    "\n",
    "news['text'] = news['headline'].map(str)+ '. ' +news['short_description']\n",
    "news['clean text'] = normalize_corpus(news['text'],remove_digits=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0ca62fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining the x and y\n",
    "x = news['clean text']\n",
    "y = news.label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dae2b9ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'crenezumab trial will gauge whether alzheimer s drug can prevent or slow the disease the clock be tick for the united states to find a cure the team be work on the study with dr francisco lopera of'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "132f016c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b0b3d659",
   "metadata": {},
   "outputs": [],
   "source": [
    "# spliting the data for modelling purpose...\n",
    "trainx , testx , trainy , testy = train_test_split(x,y,random_state=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "91297330",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((37500,), (12500,), (37500,), (12500,))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking shape of the splitted data...\n",
    "trainx.shape, testx.shape , trainy.shape , testy.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ad7cc652",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((37500, 36118), (12500, 36118))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use count vectoriser technique to create the dtm for train and test data...\n",
    "vect = CountVectorizer()\n",
    "trainx_dtm = vect.fit_transform(trainx)\n",
    "testx_dtm = vect.transform(testx)\n",
    "trainx_dtm.shape,testx_dtm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6dd67c54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['zocdoc', 'zoe', 'zoellick', 'zoey', 'zolpidem', 'zoltan',\n",
       "       'zoltar', 'zombie', 'zomnir', 'zone', 'zong', 'zoo', 'zoodle',\n",
       "       'zooey', 'zookeeper', 'zoolander', 'zoom', 'zootopia', 'zor',\n",
       "       'zosia', 'zoubeir', 'zouhour', 'zoukis', 'zsa', 'zubairy',\n",
       "       'zubaydah', 'zubaydeh', 'zuberi', 'zuburbia', 'zucchini',\n",
       "       'zucchinis', 'zuck', 'zucker', 'zuckerberg', 'zulily', 'zulu',\n",
       "       'zuma', 'zumba', 'zuravleff', 'zurich', 'zurlon', 'zusak',\n",
       "       'zuzana', 'zuzu', 'zwigoff', 'zylka', 'zymurgy', 'zynga', 'zyola',\n",
       "       'zywicki'], dtype=object)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking the last 50 features names...\n",
    "vect.get_feature_names_out()[-50:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0bda3242",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(37500, 36118)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainx_dtm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f8c4e205",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the same technique but lets take one hyperparameter lowercase = false to not to convert to lowercase...\n",
    "vect = CountVectorizer(lowercase=False)\n",
    "trainx_dtm_NL = vect.fit_transform(trainx)\n",
    "testx_dtm_NL = vect.transform(testx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2c4beb91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(37500, 36118)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainx_dtm_NL.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1e26e2e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets include 1-ngrams and 2 ngrams...\n",
    "vect = CountVectorizer(ngram_range=(1,2))\n",
    "trainx_dtm_ngm = vect.fit_transform(trainx)\n",
    "testx_dtm_ngm = vect.transform(testx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0d832c63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(37500, 481824)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainx_dtm_ngm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ed931361",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['zuckerberg post', 'zuckerberg say', 'zuckerberg speak',\n",
       "       'zuckerberg to', 'zuckerberg wife', 'zulily', 'zulily qa', 'zulu',\n",
       "       'zulu african', 'zuma', 'zuma from', 'zuma resign', 'zuma south',\n",
       "       'zumba', 'zumba class', 'zumba meet', 'zumba your', 'zuravleff',\n",
       "       'zuravleff show', 'zurich', 'zurich be', 'zurich beautiful',\n",
       "       'zurich chess', 'zurich local', 'zurich look', 'zurich mosque',\n",
       "       'zurich name', 'zurich on', 'zurich recommend', 'zurich the',\n",
       "       'zurlon', 'zurlon tipton', 'zusak', 'zusak much', 'zuzana',\n",
       "       'zuzana navelkova', 'zuzu', 'zuzu joyfully', 'zwigoff',\n",
       "       'zwigoff claim', 'zylka', 'zylka um', 'zymurgy',\n",
       "       'zymurgy magazine', 'zynga', 'zynga ceo', 'zyola', 'zyola mix',\n",
       "       'zywicki', 'zywicki be'], dtype=object)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect.get_feature_names_out()[-50:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cfcb12fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.77976"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use naive bayes to predict the news category and evaluate the models accuracy...\n",
    "model = MultinomialNB()\n",
    "model.fit(trainx_dtm_ngm,trainy)\n",
    "pred_class = model.predict(testx_dtm_ngm)\n",
    "accuracy_score(testy,pred_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "044a4590",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5, 7, 0, ..., 4, 1, 8], dtype=int64)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# predicted categories...\n",
    "pred_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "892aa18d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.89952"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate null accuracy...\n",
    "y_test_binary = np.where(testy==5, 1, 0)\n",
    "max(y_test_binary.mean(), 1 - y_test_binary.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "eb55bba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets create a function which transform the data into dtm and do modelling and give the accuracy...\n",
    "def tokenise_test(vect):\n",
    "    trainx_dtm = vect.fit_transform(trainx)\n",
    "    print('Features:',trainx_dtm.shape[1])\n",
    "    testx_dtm = vect.transform(testx)\n",
    "    nb = MultinomialNB()\n",
    "    nb.fit(trainx_dtm,trainy)\n",
    "    pred_class = nb.predict(testx_dtm)\n",
    "    print('Accuracy:',accuracy_score(testy,pred_class))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e35a2917",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features: 36118\n",
      "Accuracy: 0.79088\n"
     ]
    }
   ],
   "source": [
    "# include 1-grams and 1-grams\n",
    "tokenise_test(CountVectorizer(ngram_range=(1,1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "65f1cab1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features: 481824\n",
      "Accuracy: 0.77976\n"
     ]
    }
   ],
   "source": [
    "# include 1-grams and 2-grams\n",
    "tokenise_test(CountVectorizer(ngram_range=(1,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "59594143",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features: 36118\n",
      "Accuracy: 0.79088\n"
     ]
    }
   ],
   "source": [
    "# include 1-grams and 1-grams and lower = false...\n",
    "tokenise_test(CountVectorizer(ngram_range=(1,1),lowercase=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "46192311",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove stopwords...\n",
    "vect = CountVectorizer(stop_words='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4e6101b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "frozenset({'neither', 'yourself', 'anywhere', 'top', 'he', 'yourselves', 'along', 'bottom', 'side', 'name', 'found', 'sometimes', 'both', 'couldnt', 'too', 'rather', 'among', 'whole', 'still', 'their', 'hasnt', 'co', 'this', 'everything', 'ourselves', 'there', 'been', 'fill', 'eight', 'next', 'moreover', 'myself', 'someone', 'move', 'thus', 'formerly', 'cant', 'however', 'six', 'onto', 'anyway', 'so', 'during', 'part', 'where', 'became', 'alone', 'these', 'elsewhere', 'due', 'must', 'well', 'behind', 'mill', 'bill', 'describe', 'everyone', 'my', 'whither', 'could', 'for', 'toward', 'everywhere', 'over', 'has', 'therefore', 'without', 'from', 'thru', 'twelve', 'because', 'eleven', 'least', 'being', 'sincere', 'beside', 'serious', 'done', 'via', 'his', 'will', 'meanwhile', 'own', 'third', 'amount', 'had', 'they', 'always', 'ours', 'down', 'am', 'perhaps', 'detail', 'thence', 'former', 'a', 'how', 'four', 'fifty', 'nine', 'some', 'interest', 'somehow', 'becoming', 'etc', 'take', 'somewhere', 'is', 'be', 'first', 'empty', 'whoever', 'might', 'amoungst', 'further', 'inc', 'any', 'other', 'three', 'thereupon', 'call', 'most', 'thereby', 'ten', 'if', 'con', 'get', 'sometime', 'of', 'again', 'before', 'your', 'you', 'no', 'each', 'afterwards', 'its', 'do', 'below', 'almost', 'hereafter', 'beforehand', 'hereby', 'an', 'often', 'until', 'system', 'on', 'something', 'amongst', 'latterly', 'two', 'beyond', 'un', 'fifteen', 'nevertheless', 'fire', 'please', 'much', 'ie', 'another', 'ever', 'whereupon', 'why', 'otherwise', 'here', 'then', 'me', 'within', 'now', 'twenty', 'i', 'thin', 'indeed', 'hundred', 'front', 'back', 'once', 'either', 'them', 'with', 'others', 'was', 'towards', 'less', 'she', 'eg', 'who', 'at', 'by', 'none', 'enough', 'many', 'against', 'thereafter', 'into', 'herself', 'already', 'per', 'wherein', 'yours', 'same', 'to', 'about', 'mine', 'becomes', 'even', 'across', 'whether', 'whom', 'last', 'all', 'see', 'after', 'few', 'very', 'show', 'one', 'anyhow', 'noone', 'themselves', 'latter', 'nobody', 'under', 'cannot', 'or', 're', 'are', 'while', 'cry', 'hereupon', 'else', 'have', 'five', 'we', 'whereafter', 'herein', 'more', 'keep', 'seemed', 'but', 'give', 'put', 'seems', 'anyone', 'nor', 'him', 'only', 'together', 'full', 'not', 'in', 'whence', 'thick', 'forty', 'when', 'around', 'that', 'mostly', 'except', 'than', 'every', 'though', 'go', 'yet', 'our', 'out', 'nothing', 'those', 'up', 'throughout', 'de', 'through', 'such', 'whereas', 'himself', 'itself', 'anything', 'never', 'made', 'since', 'nowhere', 'whatever', 'would', 'seeming', 'wherever', 'should', 'as', 'may', 'although', 'ltd', 'above', 'the', 'seem', 'whenever', 'were', 'us', 'whose', 'and', 'what', 'also', 'her', 'it', 'become', 'between', 'besides', 'can', 'find', 'which', 'whereby', 'upon', 'several', 'therein', 'sixty', 'hers', 'off', 'hence', 'namely'})\n"
     ]
    }
   ],
   "source": [
    "# set of stop words...\n",
    "print(vect.get_stop_words())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "02223fcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features: 35826\n",
      "Accuracy: 0.8036\n"
     ]
    }
   ],
   "source": [
    "# use hyperparameter stop_words='english'\n",
    "tokenise_test(CountVectorizer(stop_words='english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ee0d6546",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features: 36118\n",
      "Accuracy: 0.79088\n"
     ]
    }
   ],
   "source": [
    "# without hyperparameter stop_words...\n",
    "tokenise_test(CountVectorizer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1d76ff87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features: 467531\n",
      "Accuracy: 0.8224\n"
     ]
    }
   ],
   "source": [
    "# use all three hyperparameter which we did earlier...\n",
    "tokenise_test(CountVectorizer(stop_words='english',ngram_range=(1,2),lowercase=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d7c15b1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features: 100\n",
      "Accuracy: 0.41624\n"
     ]
    }
   ],
   "source": [
    "# remove English stop words and only keep 100 features(max_features=100)\n",
    "tokenise_test(CountVectorizer(stop_words='english',max_features=100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ce91cbfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features: 2000\n",
      "Accuracy: 0.7396\n"
     ]
    }
   ],
   "source": [
    "tokenise_test(CountVectorizer(stop_words='english',max_features=2000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "00410005",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features: 3000\n",
      "Accuracy: 0.75704\n"
     ]
    }
   ],
   "source": [
    "tokenise_test(CountVectorizer(stop_words='english',max_features=3000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7f4ebedd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features: 6000\n",
      "Accuracy: 0.78712\n"
     ]
    }
   ],
   "source": [
    "tokenise_test(CountVectorizer(stop_words='english',max_features=6000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "03bd1b10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features: 19958\n",
      "Accuracy: 0.226\n"
     ]
    }
   ],
   "source": [
    "# include 1-grams and 2-grams, and only include terms that appear at least 2 times\n",
    "tokenise_test(CountVectorizer(stop_words='english',max_df=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "65ca8050",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features: 13193\n",
      "Accuracy: 0.79888\n"
     ]
    }
   ],
   "source": [
    "tokenise_test(CountVectorizer(stop_words='english',min_df=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "1da3cf39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features: 11442\n",
      "Accuracy: 0.79784\n"
     ]
    }
   ],
   "source": [
    "tokenise_test(CountVectorizer(stop_words='english',min_df=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "7900b998",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features: 10147\n",
      "Accuracy: 0.79496\n"
     ]
    }
   ],
   "source": [
    "tokenise_test(CountVectorizer(stop_words='english',min_df=6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "60662d4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features: 9165\n",
      "Accuracy: 0.7932\n"
     ]
    }
   ],
   "source": [
    "tokenise_test(CountVectorizer(stop_words='english',min_df=7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "7627f8d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Let's use tf-idf vectoriser..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a11d968e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features: 35826\n",
      "Accuracy: 0.80312\n"
     ]
    }
   ],
   "source": [
    "# use tf-idf with stop words...\n",
    "tokenise_test(TfidfVectorizer(stop_words='english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "2c92fbdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features: 35826\n",
      "Accuracy: 0.80312\n"
     ]
    }
   ],
   "source": [
    "# lets use the updated stopwords which we created earlier...\n",
    "tokenise_test(TfidfVectorizer(stop_words='english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "4ba70502",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features: 467531\n",
      "Accuracy: 0.8224\n"
     ]
    }
   ],
   "source": [
    "vect = CountVectorizer(stop_words='english',ngram_range=(1,2),lowercase=False)\n",
    "trainx_dtm = vect.fit_transform(trainx)\n",
    "print('Features:',trainx_dtm.shape[1])\n",
    "testx_dtm = vect.transform(testx)\n",
    "nb = MultinomialNB()\n",
    "nb.fit(trainx_dtm,trainy)\n",
    "pred_class = nb.predict(testx_dtm)\n",
    "print('Accuracy:',accuracy_score(testy,pred_class))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "9ad3c07a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      " [[ 989   13   11   27   21  111   43    9   43    6]\n",
      " [  23  964   11   20    6   34    0   87   85   10]\n",
      " [  28   34  934   25   69   74    5   13   24   29]\n",
      " [  26    9   21 1037   19   34   36   28   20    7]\n",
      " [  36    3   15   26 1090   41   11    3   12    4]\n",
      " [ 103   15   23   24   23 1011   17    5   26    9]\n",
      " [  59    5   16   70   21   39 1000    1   16    5]\n",
      " [  17   58   12   30    6   21    0 1067   60   14]\n",
      " [  49   43    7   27    9   40   10   15 1067    9]\n",
      " [   7   14   10   11    7   23    4   13   15 1121]]\n"
     ]
    }
   ],
   "source": [
    "print('Confusion Matrix:\\n',confusion_matrix(testy,pred_class))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "bd8505bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.78      0.76      1273\n",
      "           1       0.83      0.78      0.80      1240\n",
      "           2       0.88      0.76      0.81      1235\n",
      "           3       0.80      0.84      0.82      1237\n",
      "           4       0.86      0.88      0.87      1241\n",
      "           5       0.71      0.80      0.75      1256\n",
      "           6       0.89      0.81      0.85      1232\n",
      "           7       0.86      0.83      0.84      1285\n",
      "           8       0.78      0.84      0.81      1276\n",
      "           9       0.92      0.92      0.92      1225\n",
      "\n",
      "    accuracy                           0.82     12500\n",
      "   macro avg       0.83      0.82      0.82     12500\n",
      "weighted avg       0.83      0.82      0.82     12500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Classification Report:\\n',classification_report(testy,pred_class))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "cd23e2a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score: 0.8224\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy Score:',accuracy_score(testy,pred_class))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d01d7545",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac909960",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "53115ca3",
   "metadata": {},
   "source": [
    "### Deployment Process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "87dbb4f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "d23acbe6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved as 'Normalisecorpus.pkl' \n"
     ]
    }
   ],
   "source": [
    "with open('Normalisecorpus.pkl','wb') as file:\n",
    "    pickle.dump(normalize_corpus,file)\n",
    "print('''Model saved as 'Normalisecorpus.pkl' ''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "89dda01e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved as 'countvectoriser.pkl' \n"
     ]
    }
   ],
   "source": [
    "with open('countvectoriser.pkl','wb') as file:\n",
    "    pickle.dump(vect,file)\n",
    "print('''Model saved as 'countvectoriser.pkl' ''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "208ba415",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved as 'modelnb.pkl' \n"
     ]
    }
   ],
   "source": [
    "with open('modelnb.pkl','wb') as file:\n",
    "    pickle.dump(nb,file)\n",
    "print('''Model saved as 'modelnb.pkl' ''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "a0ec8b41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IMPORTANT: You are using gradio version 4.7.1, however version 4.29.0 is available, please upgrade.\n",
      "--------\n",
      "Running on local URL:  http://127.0.0.1:7861\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7861/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "import pickle\n",
    "import unicodedata\n",
    "import re\n",
    "import spacy\n",
    "\n",
    "# Loading the vectorizer, model, and SpaCy NLP model\n",
    "with open('countvectoriser.pkl', 'rb') as f:\n",
    "    vect = pickle.load(f)\n",
    "\n",
    "with open('modelnb.pkl', 'rb') as f:\n",
    "    nb = pickle.load(f)\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\") \n",
    "\n",
    "# Category mapping dictionary\n",
    "categories = {'WELLNESS': 0, 'POLITICS': 1, 'ENTERTAINMENT': 2, 'TRAVEL': 3,\n",
    "              'STYLE & BEAUTY': 4, 'PARENTING': 5, 'FOOD & DRINK': 6, 'WORLD NEWS': 7,\n",
    "              'BUSINESS': 8, 'SPORTS': 9}\n",
    "reverse_categories = {v: k for k, v in categories.items()}\n",
    "\n",
    "# Text cleaning functions\n",
    "def remove_accented_chars(text):\n",
    "    return unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "\n",
    "def remove_special_characters(text, remove_digits=False):\n",
    "    pattern = r'[^a-zA-z0-9\\s]' if not remove_digits else r'[^a-zA-z\\s]'\n",
    "    return re.sub(pattern, '', text)\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    doc = nlp(text)\n",
    "    return ' '.join([word.lemma_ if word.lemma_ != '-PRON-' else word.text for word in doc])\n",
    "\n",
    "# Corpus normalization\n",
    "def normalize_corpus(corpus, accented_char_removal=True, text_lower_case=True, \n",
    "                     text_lemmatization=True, special_char_removal=True, remove_digits=True):\n",
    "    \n",
    "    normalized_corpus = []\n",
    "    for doc in corpus:\n",
    "        if accented_char_removal:\n",
    "            doc = remove_accented_chars(doc)\n",
    "        if text_lower_case:\n",
    "            doc = doc.lower()\n",
    "        doc = re.sub(r'[\\r|\\n|\\r\\n]+', ' ', doc)\n",
    "        if text_lemmatization:\n",
    "            doc = lemmatize_text(doc)\n",
    "        if special_char_removal:\n",
    "            special_char_pattern = re.compile(r'([{.(-)!}])')\n",
    "            doc = special_char_pattern.sub(\" \\\\1 \", doc)\n",
    "            doc = remove_special_characters(doc, remove_digits=remove_digits)\n",
    "        doc = re.sub(' +', ' ', doc)\n",
    "        normalized_corpus.append(doc)\n",
    "        \n",
    "    return normalized_corpus\n",
    "\n",
    "# Prediction function\n",
    "def classify_text(text):\n",
    "    normalized_text = normalize_corpus([text])\n",
    "    input_vector = vect.transform(normalized_text)\n",
    "    prediction = nb.predict(input_vector)\n",
    "    return reverse_categories[prediction[0]]\n",
    "\n",
    "# Create the Gradio interface\n",
    "iface = gr.Interface(fn=classify_text, inputs=\"text\", outputs=\"text\", title=\"Text Classification\")\n",
    "\n",
    "# Launch the interface locally to check\n",
    "iface.launch()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ca1f88b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "927fd092",
   "metadata": {},
   "source": [
    "## Model Deployed :\n",
    "* **You can try it by clicking the link**\n",
    "https://huggingface.co/spaces/HarshU1/News-Category-Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e98e749",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a7356fae",
   "metadata": {},
   "source": [
    "## Yelp star rating prediction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "a4680666",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing necessary libraries...\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# for splitting the data...\n",
    "from sklearn.model_selection import train_test_split \n",
    "\n",
    "# for modelling...\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# for evaluate the model...\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# for ignoring warnings...\n",
    "import warnings \n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "16a8b23f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>business_id</th>\n",
       "      <th>date</th>\n",
       "      <th>review_id</th>\n",
       "      <th>stars</th>\n",
       "      <th>text</th>\n",
       "      <th>type</th>\n",
       "      <th>user_id</th>\n",
       "      <th>cool</th>\n",
       "      <th>useful</th>\n",
       "      <th>funny</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9yKzy9PApeiPPOUJEtnvkg</td>\n",
       "      <td>2011-01-26</td>\n",
       "      <td>fWKvX83p0-ka4JS3dc6E5A</td>\n",
       "      <td>5</td>\n",
       "      <td>My wife took me here on my birthday for breakf...</td>\n",
       "      <td>review</td>\n",
       "      <td>rLtl8ZkDX5vH5nAx9C3q5Q</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ZRJwVLyzEJq1VAihDhYiow</td>\n",
       "      <td>2011-07-27</td>\n",
       "      <td>IjZ33sJrzXqU-0X6U8NwyA</td>\n",
       "      <td>5</td>\n",
       "      <td>I have no idea why some people give bad review...</td>\n",
       "      <td>review</td>\n",
       "      <td>0a2KyEL0d3Yb1V6aivbIuQ</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              business_id        date               review_id  stars  \\\n",
       "0  9yKzy9PApeiPPOUJEtnvkg  2011-01-26  fWKvX83p0-ka4JS3dc6E5A      5   \n",
       "1  ZRJwVLyzEJq1VAihDhYiow  2011-07-27  IjZ33sJrzXqU-0X6U8NwyA      5   \n",
       "\n",
       "                                                text    type  \\\n",
       "0  My wife took me here on my birthday for breakf...  review   \n",
       "1  I have no idea why some people give bad review...  review   \n",
       "\n",
       "                  user_id  cool  useful  funny  \n",
       "0  rLtl8ZkDX5vH5nAx9C3q5Q     2       5      0  \n",
       "1  0a2KyEL0d3Yb1V6aivbIuQ     0       0      0  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the data yelp reviews...\n",
    "yelp = pd.read_csv('yelp.csv')\n",
    "yelp.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ac7772ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extracting the best review and worst review...\n",
    "yelp_best_worst = yelp[(yelp.stars==5) | (yelp.stars==1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0ca62fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining the x and y\n",
    "x = yelp_best_worst.text\n",
    "y = yelp_best_worst.stars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b0b3d659",
   "metadata": {},
   "outputs": [],
   "source": [
    "# spliting the data for modelling purpose...\n",
    "trainx , testx , trainy , testy = train_test_split(x,y,random_state=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "91297330",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3064,), (1022,), (3064,), (1022,))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking shape of the splitted data...\n",
    "trainx.shape, testx.shape , trainy.shape , testy.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ad7cc652",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use count vectoriser technique to create the dtm for train and test data...\n",
    "vect = CountVectorizer()\n",
    "trainx_dtm = vect.fit_transform(trainx)\n",
    "testx_dtm = vect.transform(testx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6dd67c54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['yung', 'yup', 'yur', 'yuri', 'yusefs', 'yuukk', 'yuuuummmmae',\n",
       "       'yuuuuuuum', 'yyyyy', 'z11', 'za', 'zabba', 'zach', 'zam',\n",
       "       'zatsiki', 'zen', 'zero', 'zesty', 'zha', 'zia', 'zichini',\n",
       "       'zihuatenejo', 'zilch', 'zin', 'zinburger', 'zinburgergeist',\n",
       "       'zinc', 'zinfandel', 'zing', 'zip', 'zipcar', 'zippers', 'zipps',\n",
       "       'zoe', 'zombies', 'zone', 'zones', 'zoning', 'zoo', 'zoom',\n",
       "       'zucca', 'zucchini', 'zuccini', 'zuchinni', 'zumba', 'zupa',\n",
       "       'zupas', 'zuzu', 'zuzus', 'zzed'], dtype=object)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking the last 50 features names...\n",
    "vect.get_feature_names_out()[-50:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0bda3242",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3064, 16757)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainx_dtm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f8c4e205",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the same technique but lets take one hyperparameter lowercase = false to not to convert to lowercase...\n",
    "vect = CountVectorizer(lowercase=False)\n",
    "trainx_dtm_NL = vect.fit_transform(trainx)\n",
    "testx_dtm_NL = vect.transform(testx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2c4beb91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3064, 20717)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainx_dtm_NL.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1e26e2e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets include 1-ngrams and 2 ngrams...\n",
    "vect = CountVectorizer(ngram_range=(1,2))\n",
    "trainx_dtm_ngm = vect.fit_transform(trainx)\n",
    "testx_dtm_ngm = vect.transform(testx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0d832c63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3064, 169121)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainx_dtm_ngm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ed931361",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['zone of', 'zone out', 'zones', 'zones dolls', 'zoning',\n",
       "       'zoning issues', 'zoo', 'zoo and', 'zoo if', 'zoo is', 'zoo the',\n",
       "       'zoo tour', 'zoo ve', 'zoom', 'zoom in', 'zucca',\n",
       "       'zucca appetizer', 'zucchini', 'zucchini and', 'zucchini bread',\n",
       "       'zucchini broccoli', 'zucchini carrots', 'zucchini fires',\n",
       "       'zucchini fries', 'zucchini pieces', 'zucchini strips',\n",
       "       'zucchini veal', 'zucchini very', 'zucchini we', 'zucchini with',\n",
       "       'zuccini', 'zuccini italian', 'zuchinni', 'zuchinni the', 'zumba',\n",
       "       'zumba or', 'zumba yogalates', 'zupa', 'zupa flavors', 'zupas',\n",
       "       'zupas cater', 'zuzu', 'zuzu in', 'zuzu is', 'zuzu the',\n",
       "       'zuzu was', 'zuzus', 'zuzus room', 'zzed', 'zzed in'], dtype=object)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect.get_feature_names_out()[-50:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "cfcb12fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8434442270058709"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use naive bayes to predict the star rating and evaluate the models accuracy...\n",
    "model = MultinomialNB()\n",
    "model.fit(trainx_dtm_ngm,trainy)\n",
    "pred_class = model.predict(testx_dtm_ngm)\n",
    "accuracy_score(testy,pred_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "044a4590",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5, 5, 5, ..., 5, 5, 5], dtype=int64)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# predicted stars...\n",
    "pred_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "892aa18d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8140900195694716"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate null accuracy...\n",
    "y_test_binary = np.where(testy==5, 1, 0)\n",
    "max(y_test_binary.mean(), 1 - y_test_binary.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "eb55bba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets create a function which transform the data into dtm and do modelling and give the accuracy...\n",
    "def tokenise_test(vect):\n",
    "    trainx_dtm = vect.fit_transform(trainx)\n",
    "    print('Features:',trainx_dtm.shape[1])\n",
    "    testx_dtm = vect.transform(testx)\n",
    "    nb = MultinomialNB()\n",
    "    nb.fit(trainx_dtm,trainy)\n",
    "    pred_class = nb.predict(testx_dtm)\n",
    "    print('Accuracy:',accuracy_score(testy,pred_class))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e35a2917",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features: 16757\n",
      "Accuracy: 0.9324853228962818\n"
     ]
    }
   ],
   "source": [
    "# include 1-grams and 1-grams\n",
    "tokenise_test(CountVectorizer(ngram_range=(1,1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "65f1cab1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features: 169121\n",
      "Accuracy: 0.8434442270058709\n"
     ]
    }
   ],
   "source": [
    "# include 1-grams and 2-grams\n",
    "tokenise_test(CountVectorizer(ngram_range=(1,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "59594143",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features: 20717\n",
      "Accuracy: 0.9197651663405088\n"
     ]
    }
   ],
   "source": [
    "# include 1-grams and 1-grams and lower = false...\n",
    "tokenise_test(CountVectorizer(ngram_range=(1,1),lowercase=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "46192311",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove stopwords...\n",
    "vect = CountVectorizer(stop_words='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "4e6101b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "frozenset({'none', 'them', 'ever', 'twelve', 'very', 'someone', 'become', 'almost', 'without', 'am', 'any', 'take', 'same', 'de', 'must', 'keep', 'amount', 'beforehand', 'against', 'thence', 'former', 'no', 'everyone', 'own', 'done', 'may', 'already', 'ours', 'in', 'down', 'there', 'thick', 'sixty', 'whither', 'many', 'nothing', 'seem', 'those', 'whereby', 'because', 'ten', 'whereas', 'since', 'were', 'enough', 'has', 'hereby', 'towards', 'me', 'with', 'who', 'formerly', 'beside', 'is', 'go', 'only', 'much', 'front', 'his', 'do', 'noone', 'anyone', 'whether', 'anyway', 'through', 'yourself', 'if', 'thereupon', 'yet', 'nine', 'i', 'hasnt', 'either', 'whence', 'across', 'hers', 'indeed', 'hundred', 'over', 'about', 'was', 'had', 'thereafter', 'too', 'elsewhere', 'hereafter', 'give', 'nevertheless', 'a', 'whole', 'even', 'but', 'fill', 'yours', 'anything', 'its', 'onto', 'five', 'forty', 'everything', 'upon', 'besides', 'interest', 'wherein', 'get', 'herein', 'seeming', 'on', 'here', 'mine', 'often', 'side', 'should', 'beyond', 'again', 'eight', 'up', 'of', 'others', 'describe', 'they', 'less', 'by', 'below', 'under', 'so', 'next', 'bottom', 'their', 'at', 'to', 'when', 'call', 'last', 'she', 'else', 'nowhere', 'more', 'four', 'other', 'fire', 'an', 'inc', 'my', 'still', 'cry', 'system', 'found', 'though', 'thus', 'might', 'and', 'wherever', 'made', 'we', 'seems', 'into', 'you', 'among', 'themselves', 'bill', 'serious', 'why', 'himself', 'whereafter', 'cant', 'afterwards', 'latter', 'as', 'find', 'whom', 'namely', 'becomes', 'around', 'well', 'most', 'off', 'both', 'nor', 'ourselves', 'due', 'sincere', 'couldnt', 'please', 'two', 'although', 'another', 'therefore', 'be', 'such', 'third', 'him', 'for', 'nobody', 'mostly', 'between', 'whenever', 'put', 'otherwise', 'somehow', 'seemed', 're', 'thereby', 'via', 'yourselves', 'during', 'could', 'full', 'latterly', 'above', 'our', 'it', 'un', 'eg', 'would', 'been', 'top', 'back', 'along', 'ltd', 'several', 'everywhere', 'thru', 'anywhere', 'he', 'except', 'anyhow', 'toward', 'eleven', 'etc', 'meanwhile', 'least', 'behind', 'whose', 'each', 'one', 'out', 'part', 'her', 'thin', 'every', 'or', 'move', 'together', 'per', 'cannot', 'twenty', 'empty', 'sometimes', 'now', 'sometime', 'whatever', 'fifteen', 'whereupon', 'first', 'became', 'hence', 'fifty', 'us', 'which', 'always', 'alone', 'amongst', 'until', 'detail', 'will', 'amoungst', 'being', 'rather', 'itself', 'something', 'whoever', 'show', 'some', 'three', 'are', 'that', 'throughout', 'see', 'your', 'therein', 'after', 'somewhere', 'this', 'co', 'six', 'con', 'then', 'these', 'never', 'mill', 'from', 'herself', 'not', 'neither', 'ie', 'name', 'once', 'however', 'than', 'perhaps', 'hereupon', 'all', 'before', 'within', 'further', 'also', 'where', 'what', 'becoming', 'can', 'while', 'have', 'myself', 'how', 'few', 'the', 'moreover'})\n"
     ]
    }
   ],
   "source": [
    "# set of stop words...\n",
    "print(vect.get_stop_words())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "02223fcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features: 16460\n",
      "Accuracy: 0.9217221135029354\n"
     ]
    }
   ],
   "source": [
    "# use hyperparameter stop_words='english'\n",
    "tokenise_test(CountVectorizer(stop_words='english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ee0d6546",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features: 16757\n",
      "Accuracy: 0.9324853228962818\n"
     ]
    }
   ],
   "source": [
    "# without hyperparameter stop_words...\n",
    "tokenise_test(CountVectorizer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "1d76ff87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features: 165232\n",
      "Accuracy: 0.8405088062622309\n"
     ]
    }
   ],
   "source": [
    "# use all three hyperparameter which we did earlier...\n",
    "tokenise_test(CountVectorizer(stop_words='english',ngram_range=(1,2),lowercase=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "666e0fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets increase the set of stop words...\n",
    "from sklearn.feature_extraction import text\n",
    "stop_words = text.ENGLISH_STOP_WORDS.union(['abcd'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "8c82d86c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features: 16460\n",
      "Accuracy: 0.9217221135029354\n"
     ]
    }
   ],
   "source": [
    "# check the model at updated stopwords...\n",
    "tokenise_test(CountVectorizer(stop_words=list(stop_words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "d7c15b1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features: 100\n",
      "Accuracy: 0.87279843444227\n"
     ]
    }
   ],
   "source": [
    "# remove English stop words and only keep 100 features(max_features=100)\n",
    "tokenise_test(CountVectorizer(stop_words=list(stop_words),max_features=100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "ce91cbfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features: 1000\n",
      "Accuracy: 0.9119373776908023\n"
     ]
    }
   ],
   "source": [
    "tokenise_test(CountVectorizer(stop_words=list(stop_words),max_features=1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "00410005",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features: 500\n",
      "Accuracy: 0.8913894324853229\n"
     ]
    }
   ],
   "source": [
    "tokenise_test(CountVectorizer(stop_words='english',max_features=500))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "03bd1b10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features: 10394\n",
      "Accuracy: 0.8003913894324853\n"
     ]
    }
   ],
   "source": [
    "# include 1-grams and 2-grams, and only include terms that appear at least 2 times\n",
    "tokenise_test(CountVectorizer(stop_words=list(stop_words),max_df=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "65ca8050",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features: 4896\n",
      "Accuracy: 0.9266144814090019\n"
     ]
    }
   ],
   "source": [
    "tokenise_test(CountVectorizer(stop_words=list(stop_words),min_df=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "1da3cf39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features: 3554\n",
      "Accuracy: 0.9217221135029354\n"
     ]
    }
   ],
   "source": [
    "tokenise_test(CountVectorizer(stop_words=list(stop_words),min_df=6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "7627f8d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Let's use tf-idf vectoriser..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "a11d968e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features: 16460\n",
      "Accuracy: 0.8140900195694716\n"
     ]
    }
   ],
   "source": [
    "# use tf-idf with stop words...\n",
    "tokenise_test(TfidfVectorizer(stop_words='english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "2c92fbdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features: 16460\n",
      "Accuracy: 0.8140900195694716\n"
     ]
    }
   ],
   "source": [
    "# lets use the updated stopwords which we created earlier...\n",
    "tokenise_test(TfidfVectorizer(stop_words=list(stop_words)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f61edbf",
   "metadata": {},
   "source": [
    "### Adding Features to a Document-Term Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "f7d973a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining x and y and split the data into train and test...\n",
    "x = yelp_best_worst[['text','cool','useful','funny']]\n",
    "y = yelp_best_worst['stars']\n",
    "\n",
    "xtrain , xtest , ytrain , ytest = train_test_split(x,y,random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "38e4339b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3064, 4), (1022, 4), (3064,), (1022,))"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the shape of the train and test data....\n",
    "xtrain.shape,xtest.shape,ytrain.shape,ytest.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "1ef3ea9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy as sp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "14c1245d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3064, 16825)\n",
      "(1022, 16825)\n"
     ]
    }
   ],
   "source": [
    "# use countvectoriser for text column only...\n",
    "vect = CountVectorizer()\n",
    "xtrain_dtm = vect.fit_transform(xtrain.text)\n",
    "xtest_dtm = vect.transform(xtest.text)\n",
    "print(xtrain_dtm.shape)\n",
    "print(xtest_dtm.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "9098b770",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3064, 3)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cast other feature columns to float and convert to a sparse matrix\n",
    "extra = sp.sparse.csr_matrix(xtrain.drop('text',axis=1)).astype(float)\n",
    "extra.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "cc9db43d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3064, 16828)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# combine sparse matrices\n",
    "xtrain_dtm_extra = sp.sparse.hstack((xtrain_dtm,extra))\n",
    "xtrain_dtm_extra.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "3dbc8f45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1022, 16828)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# repeat for testing set\n",
    "extra_test = sp.sparse.csr_matrix(xtest.drop('text',axis=1)).astype(float)\n",
    "xtest_dtm_extra_test = sp.sparse.hstack((xtest_dtm,extra_test))\n",
    "xtest_dtm_extra_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "a5368447",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9256360078277887"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use logistic regression with text column only\n",
    "\n",
    "logreg = LogisticRegression()\n",
    "logreg.fit(xtrain_dtm,ytrain)\n",
    "pred_class_lg = logreg.predict(xtest_dtm)\n",
    "accuracy_score(ytest,pred_class_lg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "a40a7e6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9305283757338552\n"
     ]
    }
   ],
   "source": [
    "# use logistic regression with all features\n",
    "\n",
    "logreg = LogisticRegression()\n",
    "logreg.fit(xtrain_dtm_extra, ytrain)\n",
    "pred_class_all_lg = logreg.predict(xtest_dtm_extra_test)\n",
    "print(accuracy_score(ytest, pred_class_all_lg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89dda01e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
